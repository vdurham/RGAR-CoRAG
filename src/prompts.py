from typing import List, Dict, Optional


def get_generate_subquery_prompt(query: str, 
                                 past_subqueries: List[str], 
                                 past_subanswers: List[str],
                                 extracted_facts: List[str] = None) -> List[Dict]:
    """
    Generate a subquery prompt that integrates CoRAG's iterative approach with RGAR's factual knowledge.
    
    Returns:
        messages: Prompt messages for the LLM
    """
    assert len(past_subqueries) == len(past_subanswers)
    past = ''
    for idx in range(len(past_subqueries)):
        past += f"""Intermediate query {idx+1}: {past_subqueries[idx]} \n Intermediate answer {idx + 1}: {past_subanswers[idx]}\n"""
    past = past.strip()
    
    # Format extracted facts if available
    facts_section = ""
    if extracted_facts and len(extracted_facts) > 0:
        facts_section = "## Extracted Patient Facts\n"
        for fact in extracted_facts:
            facts_section += f"- {fact}\n"
    
    prompt = f"""You are a medical search expert helping to diagnose and answer clinical questions by generating targeted search queries. Given the patient information, previously generated queries, and medical knowledge, generate a new focused follow-up question that will help answer the main clinical query.
        {facts_section}
        ## Previous intermediate queries and answers
        {past or 'Nothing yet'}
        ## Main medical query to answer
        {query}
        Generate a specific, focused follow-up question that addresses key medical details from the patient record or explores important diagnosis-related concepts mentioned in the knowledge areas. Your question should help a medical search engine find relevant clinical information for diagnosis or treatment. Respond with only the follow-up question, no explanation."""

    messages: List[Dict] = [
        {'role': 'user', 'content': prompt}
    ]
    return messages


def get_generate_intermediate_answer_prompt(subquery: str, documents: List[str]) -> List[Dict]:
    context = ''
    for idx, doc in enumerate(documents):
        context += f"""{doc}\n\n"""

    prompt = f"""Given the following documents, generate an appropriate answer for the query. DO NOT hallucinate any information, only use the provided documents to generate the answer. Respond "No relevant information found" if the documents do not contain useful information.

## Documents
{context.strip()}

## Query
{subquery}

Respond with a concise answer only, do not explain yourself or output anything else."""

    messages: List[Dict] = [
        {'role': 'user', 'content': prompt}
    ]
    return messages


def get_generate_final_answer_prompt(
        query: str, past_subqueries: List[str], past_subanswers: List[str], task_desc: str,
        documents: Optional[List[str]] = None
) -> List[Dict]:

    assert len(past_subqueries) == len(past_subanswers)
    past = ''
    for idx in range(len(past_subqueries)):
        past += f"""Intermediate query {idx+1}: {past_subqueries[idx]}
Intermediate answer {idx+1}: {past_subanswers[idx]}\n"""
    past = past.strip()

    context = ''
    if documents:
        for idx, doc in enumerate(documents):
            context += f"""Doc {idx}: {doc}\n\n"""

    prompt = f"""Given the following intermediate queries and answers, generate a final answer for the main query by combining relevant information. Note that intermediate answers are generated by an LLM and may not always be accurate.

## Documents
{context.strip()}

## Intermediate queries and answers
{past or 'Nothing yet'}

## Task description
{task_desc}

## Main query
{query}

Respond with an appropriate answer only, do not explain yourself or output anything else."""

    messages: List[Dict] = [
        {'role': 'user', 'content': prompt}
    ]
    return messages
